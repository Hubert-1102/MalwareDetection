import os
import random
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import math
import matplotlib.pyplot as plt

random.seed(1024)
required_features = {'RequestedPermission', 'UsedPermissions', 'RestrictedApi', 'SuspiciousApi'}
threshold = 2000


def get_importance_feature(benign_apps, malware_apps):
    # 对所有训练数据中的feature出现次数进行统计
    for app in benign_apps:
        with open(os.path.join(benign_path, app), 'r', encoding='utf-8') as file:
            lines = file.readlines()
            for line in lines:
                ss = line.replace('\n', '').split('List_')
                if ss[0] in required_features:
                    if ss[1] not in feature_counts:
                        feature_counts[ss[1]] = 1
                    else:
                        feature_counts[ss[1]] += 1
        file.close()
    for app in malware_apps:
        with open(os.path.join(malware_path, app), 'r', encoding='utf-8') as file:
            lines = file.readlines()
            for line in lines:
                ss = line.replace('\n', '').split('List_')
                if ss[0] in required_features:
                    if ss[1] not in feature_counts:
                        feature_counts[ss[1]] = 1
                    else:
                        feature_counts[ss[1]] += 1
        file.close()


def feature2vector(benign_apps, malware_apps, feature_map):
    cvt_vectors = []
    for app in benign_apps:
        feature = feature_map.copy()
        with open(os.path.join(benign_path, app), 'r', encoding='utf-8') as file:
            lines = file.readlines()
            for line in lines:
                ss = line.replace('\n', '').split('List_')
                if ss[1] in feature_map:
                    feature[ss[1]] = 1
        feature = list(feature.values())
        cvt_vectors.append((feature, 0))
        file.close()
    for app in malware_apps:
        feature = feature_map.copy()
        with open(os.path.join(malware_path, app), 'r', encoding='utf-8') as file:
            lines = file.readlines()
            for line in lines:
                ss = line.replace('\n', '').split('List_')
                if ss[1] in feature_map:
                    feature[ss[1]] = 1
        feature = list(feature.values())
        cvt_vectors.append((feature, 1))
        file.close()
    random.shuffle(cvt_vectors)
    return cvt_vectors


def get_feature_label(vectors):
    features = []
    labels = []
    for feature, label in vectors:
        features.append(feature)
        labels.append(label)
    l = int(len(labels) * 0.8)
    return features[:l], labels[:l], features[l:], labels[l:]


def get_sorted_features():
    important_features = sorted(feature_counts.items(), key=lambda x: -x[1])
    count = 0
    feature_map = {}
    for f, c in important_features:
        feature_map[f] = 0
        count += 1
        if count >= threshold:
            break
    return feature_map


num_features = 50


def write_dataset(train_data, train_label, test_data, test_label):
    svc = LinearSVC(dual=False)
    svc.fit(train_data + test_data, train_label + test_label)
    ordered_index = sorted(range(len(svc.coef_[0])), key=lambda k: -abs(svc.coef_[0][k]))
    with open(f'./files/%s-%s.txt' % (malware_path.split('/')[2], benign_path.split('/')[2]), 'w') as file:
        for index in range(num_features):
            print(list(feature_map.keys())[ordered_index[index]], svc.coef_[0][ordered_index[index]])
            file.write(f'%s %f %d\n' % (
                list(feature_map.keys())[ordered_index[index]], svc.coef_[0][ordered_index[index]], index + 1))
        file.close()


def classify(train_data, train_label, test_data, test_label):
    svc = LinearSVC(dual=False)
    svc.fit(train_data, train_label)
    ordered_index = sorted(range(len(svc.coef_[0])), key=lambda k: -abs(svc.coef_[0][k]))
    r_list = []
    global max_importance, total_importance
    max_importance = abs(svc.coef_[0][ordered_index[0]])
    max_importance = calculate_score(1, max_importance)
    total_importance = 0
    for i in range(1):
        for index in range(num_features):
            r_list.append(list(feature_map.keys())[ordered_index[index]])
            find_in_txt(list(feature_map.keys())[ordered_index[index]], abs(svc.coef_[0][ordered_index[index]]),
                        index + 1)
            total_importance += abs(svc.coef_[0][ordered_index[index]])
            # print(list(feature_map.keys())[ordered_index[index]], svc.coef_[0][ordered_index[index]])
        print()
        total_score = 0
        for suspicious_feature, (rank, value, score) in suspicious_features.items():
            total_score += score
            print(f'rank: {rank}       value: {value}     score: {score}      {suspicious_feature}:')
        accuracy = svc.score(test_data, test_label)
        f1 = f1_score(test_label, svc.predict(test_data))
        recall = recall_score(test_label, svc.predict(test_data))
        precision = precision_score(test_label, svc.predict(test_data))
        print(accuracy)
        print(f'{suspicious} api/permissions are found in list')
        print(
            f'{importance} feature importance, total importance is {total_importance}')
        print(f'ratio = {importance / total_importance}')
        print(f'the sum of score is {sigmoid(total_score)}')
        return sigmoid(total_score), accuracy, precision, recall, f1


def sigmoid(value: float):
    return (1 - math.exp(-value)) / (1 + math.exp(-value))


def find_in_txt(api, value, rank):
    with open('total_2010-2022_50_importance.txt', 'r') as file:
        lines = file.readlines()
        global suspicious, importance
        for line in lines:
            if api == line.split(' ')[0]:
                # if api + '\n' == line:
                suspicious += 1
                importance += value
                feature_value = float(line.split(' ')[1])
                suspicious_features[api] = (
                    rank, value, calculate_score(rank, min(feature_value, value)) / max_importance)
                break
        file.close()


def find_in_txt_test(api, value, rank, m, n):
    with open('total_2010-2022_50_importance.txt', 'r') as file:
        lines = file.readlines()
        global suspicious, importance
        for line in lines:
            if api == line.split(' ')[0]:
                # if api + '\n' == line:
                suspicious += 1
                importance += value
                feature_value = float(line.split(' ')[1])
                suspicious_features[api] = (
                    rank, value, calculate_score_test(rank, min(feature_value, value), m, n) / max_importance)
                break
        file.close()


def classify_test(train_data, train_label, test_data, test_label, m, n):
    svc = LinearSVC(dual=False)
    svc.fit(train_data, train_label)
    ordered_index = sorted(range(len(svc.coef_[0])), key=lambda k: -abs(svc.coef_[0][k]))
    r_list = []
    global max_importance, total_importance
    max_importance = abs(svc.coef_[0][ordered_index[0]])
    max_importance = calculate_score_test(1, max_importance, m, n)
    total_importance = 0
    for i in range(1):
        for index in range(num_features):
            r_list.append(list(feature_map.keys())[ordered_index[index]])
            find_in_txt_test(list(feature_map.keys())[ordered_index[index]], abs(svc.coef_[0][ordered_index[index]]),
                             index + 1, m, n)
            total_importance += abs(svc.coef_[0][ordered_index[index]])
            # print(list(feature_map.keys())[ordered_index[index]], svc.coef_[0][ordered_index[index]])
        print()
        total_score = 0
        for suspicious_feature, (rank, value, score) in suspicious_features.items():
            total_score += score
        accuracy = svc.score(test_data, test_label)
        f1 = f1_score(test_label, svc.predict(test_data))
        recall = recall_score(test_label, svc.predict(test_data))
        precision = precision_score(test_label, svc.predict(test_data))
        return sigmoid(total_score), accuracy, precision, recall, f1


def calculate_score_test(rank, value, m, n):
    score = (math.exp(1) / m) ** (-rank) * (math.exp(abs(value)+n) - 1)
    return score


def calculate_score(rank, value):
    m = 2.5
    score = m * 2 * (math.exp(1) / m) ** (-rank) * (math.exp(abs(value)) - 1)
    return score


def modify_dataset(year1, year2):
    global benign_path, benign_apps, malware_apps, malware_path, suspicious_features, total_importance, importance, feature_counts
    global suspicious, max_importance, feature_map
    importance = 0
    total_importance = 0
    suspicious_features = {}
    benign_path = f'./dataset/%dgoodware' % (year1)
    malware_path = f'./dataset/%dmalware' % (year2)
    benign_apps = os.listdir(benign_path)
    malware_apps = os.listdir(malware_path)
    feature_counts = {}
    suspicious = 0
    max_importance = 0
    get_importance_feature(benign_apps, malware_apps)
    feature_map = get_sorted_features()
    vectors = feature2vector(benign_apps, malware_apps, feature_map)
    return get_feature_label(vectors)


def modify_dataset_mix(year1, year2):
    global benign_path, benign_apps, malware_apps, malware_path, suspicious_features, total_importance, importance, feature_counts
    global suspicious, max_importance, feature_map
    importance = 0
    total_importance = 0
    suspicious_features = {}
    benign_path = f'./dataset/{year1}'
    malware_path = f'./dataset/{year2}'
    benign_apps = os.listdir(benign_path)
    malware_apps = os.listdir(malware_path)
    feature_counts = {}
    suspicious = 0
    max_importance = 0
    get_importance_feature(benign_apps, malware_apps)
    feature_map = get_sorted_features()
    vectors = feature2vector(benign_apps, malware_apps, feature_map)
    return get_feature_label(vectors)


# mode = 'dataset'
if __name__ == '__main__':
    mode = 'classify'
    if mode == 'dataset':
        for year2 in range(2010, 2023):
            for year1 in range(year2 + 1, 2023):
                importance = 0
                total_importance = 0
                suspicious_features = {}
                benign_path = f'./dataset/%dgoodware' % year1
                malware_path = f'./dataset/%dgoodware' % year2
                benign_apps = os.listdir(benign_path)
                malware_apps = os.listdir(malware_path)
                threshold = 2000
                feature_counts = {}
                suspicious = 0
                max_importance = 0
                get_importance_feature(benign_apps, malware_apps)
                feature_map = get_sorted_features()
                vectors = feature2vector(benign_apps, malware_apps, feature_map)
                train_data, train_label, test_data, test_label = get_feature_label(vectors)
                write_dataset(train_data, train_label, test_data, test_label)
    if mode == 'classify':
        timegap = []
        score_data = []
        accuracy_data = []
        scores = []
        average_score_per_time_gap = []
        average_accuracy_per_time_gap = []
        for year2 in range(2010, 2023):
            for year1 in range(2010, 2023):
                train_data, train_label, test_data, test_label = modify_dataset(year2, year1)
                score, accuracy, precision, recall, f1 = classify(train_data, train_label, test_data, test_label)
                timegap.append(abs(year1 - year2))
                scores.append((score, accuracy))
        for i in range(13):
            data1 = []
            data2 = []
            average_score = 0
            count = 0
            average_acc = 0
            for j in range(len(scores)):
                if timegap[j] == i:
                    count += 1
                    score, accuracy = scores[j]
                    data1.append(score)
                    data2.append(accuracy)
                    average_score += score
                    average_acc += accuracy
            average_score_per_time_gap.append(average_score / count)
            average_accuracy_per_time_gap.append(average_acc / count)
            accuracy_data.append(data2)
            score_data.append(data1)
        plt.plot(range(0, 13), average_score_per_time_gap)
        plt.show()
        plt.plot(range(0, 13), average_accuracy_per_time_gap)
        plt.show()
        sorted_index = sorted([x for x in range(len(scores))], key=lambda k: scores[k])
        sort_score = [x for x, y in [scores[y] for y in sorted_index]]
        sort_acc = [y for x, y in [scores[y] for y in sorted_index]]
        plt.plot(sort_score, sort_acc)
        plt.show()
        print(score_data)
        print(accuracy_data)
